{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18abe80f-6d05-4722-a7a5-6ddbf9fa5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tannson_re\n",
    "\n",
    "pd.set_option('display.max.rows', 4000)\n",
    "\n",
    "paid_customer_pat = r'PAID\\b.*?\\bCUSTOMER'\n",
    "\n",
    "paid_customer_compiler = re.compile(paid_customer_pat)\n",
    "\n",
    "no_repair_keys = [\n",
    "    'NO REPAIR', r'\\bW[^\\W_]{0,15}OUT\\b', r'\\b\\\"?NO\"?\\b[\\W_]{0,15}FIX\\b',\n",
    "    r'CUSTOMER DOESN’T WANT TO \\b(?:FIX|REPAIR|REPLACE)\\b', \n",
    "    r'SUGGEST\\b.*?\\bFOR REPAIR', 'NOT TO REPAIR', 'RETURN DEVICE', 'NO ISSUE',\n",
    "    r'\\bCANCEL\\b[^\\W]ORDER\\b', 'TRANSFER JOB', r'\\bWON\\'?T\\b\\s+BOOT\\b', 'EXPENSIVE',\n",
    "    r'\\bCAN\\'?T\\b\\s+FIX\\b', 'VOID', 'NOT WORTH', 'IS WORKING', 'DAMAGED', 'COSTLY',\n",
    "    'CANNOT FIX', 'RECYCLE', 'DIAGNOSTIC'\n",
    "]\n",
    "\n",
    "no_repair_pat = '|'.join(no_repair_keys)\n",
    "\n",
    "no_repair_compiler = re.compile(no_repair_pat)\n",
    "\n",
    "courtesy_keys = ['COURTESY', 'COMPLAIN', 'FREE', 'COMPLIMENTARY', 'WARRANTY']\n",
    "\n",
    "courtesy_pat = '|'.join(courtesy_keys)\n",
    "\n",
    "courtesy_compiler = re.compile(courtesy_pat)\n",
    "\n",
    "no_tax_pat = r'CASH|CHECK'\n",
    "\n",
    "no_tax_compiler = re.compile(no_tax_pat)\n",
    "\n",
    "def paid_customers_fnc(cell):\n",
    "    \"\"\"\n",
    "    Function references to Payment Method column.\n",
    "    Capture scenarios of having paid customers in list objects and strings.\n",
    "    Capture inclination of excluding payment method when no sale is made.\n",
    "    \"\"\"\n",
    "    if isinstance(cell, list): # Search pattern in list objects\n",
    "        return any(paid_customer_compiler.search(pym) for pym in cell)\n",
    "    if isinstance(cell, str): # Search pattern in strings\n",
    "        return bool(paid_customer_compiler.search(cell))\n",
    "    if pd.isna(cell): # Consider absence of payment method as a no-sale\n",
    "        return True\n",
    "    return False # Return everything else as False\n",
    "    \n",
    "def n_price_coherence_fnc(cell):\n",
    "    \"\"\"\n",
    "    Function references to Price column.\n",
    "    Ensure service fees are consistent with the negative totals.\n",
    "    \"\"\"\n",
    "    if isinstance(cell, list): # If list, return True if sum is negative\n",
    "        return sum(cell) <= 0\n",
    "    if isinstance(cell, float): # If float, return True if negative\n",
    "        return cell <= 0\n",
    "    if pd.isna(cell): # If value is null or missing, return True\n",
    "        return True\n",
    "    return False # Return everything else as False\n",
    "\n",
    "def base_fnc(cell, compiler):\n",
    "    \"\"\"\n",
    "    Function references to Service column for mask_no_repairs and mask_courtesies.\n",
    "    Capture scenarios where no repair was requested or needed.\n",
    "    Capture scenarios where the company gives a free repair or warranty repair.\n",
    "    \n",
    "    Function references to Payment Method column for mask_zero_tax.\n",
    "    Capture scenarios where the payment method was cash, resulting in the zero tax.\n",
    "    \"\"\"\n",
    "    if isinstance(cell, list): # Search pattern in list objects\n",
    "        return any(compiler.search(key) for key in cell)\n",
    "    if isinstance(cell, str): # Search pattern in strings\n",
    "        return bool(compiler.search(cell))\n",
    "    return False # Return everything else as False\n",
    "\n",
    "def infer_payment_fnc(row):\n",
    "    \"\"\" \n",
    "    Infer cash payment if...\n",
    "    a) zero or null tax AND\n",
    "    b) subtotal == total OR\n",
    "    c) subtotal - discount == total\n",
    "    \n",
    "    Infer credit card payment if...\n",
    "    a) tax AND\n",
    "    b) subtotal + tax - discount == total\n",
    "    \"\"\"\n",
    "    payment = row['Payment Method']\n",
    "    tax = row['Tax']\n",
    "    subtotal = row['Subtotal']\n",
    "    discount = row['Discount']\n",
    "    total = row['Total']\n",
    "\n",
    "    if pd.notna(subtotal) and pd.notna(total):\n",
    "        if (\n",
    "            (pd.isna(tax) or tax == 0)\n",
    "            and (\n",
    "                (subtotal == total) \n",
    "                or (np.isclose(subtotal - discount, total))\n",
    "            )\n",
    "        ):\n",
    "            payment = 'PAID CASH OR CHECK'\n",
    "            return payment\n",
    "        \n",
    "        if (\n",
    "            (pd.notna(tax) and tax > 0)\n",
    "            and (\n",
    "                (pd.notna(tax))\n",
    "                and (np.isclose(subtotal + tax - discount, total))\n",
    "            )\n",
    "        ):\n",
    "            payment = 'PAID CC'\n",
    "            return payment\n",
    "\n",
    "def correct_subs_and_tots(row):\n",
    "    \"\"\"\n",
    "    Function is only applicable to cash payments.\n",
    "    Replace null or zero subtotals with total value, if algebraically sound.\n",
    "    Replace null or zero totals with subtotal value, if algebraically sound.\n",
    "    \"\"\"\n",
    "    payment = row['Payment Method']\n",
    "    subtotal = row['Subtotal']\n",
    "    discount = row['Discount']\n",
    "    total = row['Total']\n",
    "\n",
    "    if re.search(no_tax_pat, payment):\n",
    "        if (pd.notna(total) and total != 0) and (pd.isna(subtotal) or subtotal == 0):\n",
    "            subtotal = total + discount\n",
    "            return pd.Series({'Subtotal': subtotal, 'Total': total})\n",
    "        if (pd.notna(subtotal) and subtotal != 0) and (pd.isna(total) or total == 0):\n",
    "            total = subtotal - discount\n",
    "            return pd.Series({'Subtotal': subtotal, 'Total': total})\n",
    "    return pd.Series({'Subtotal': subtotal, 'Total': total})\n",
    "\n",
    "def return_standardizations(entry, regex_container):\n",
    "    \"\"\"\n",
    "    Iterate through either item_dict or services_list.\n",
    "    Return the first standardized name, brand, and category if item.\n",
    "    Return the first standardized name if service.\n",
    "    \"\"\"\n",
    "    if isinstance(regex_container, dict):\n",
    "        for brand, categories in regex_container.items():\n",
    "            for category, rules in categories.items():\n",
    "                for standardized_item, compiler in rules:\n",
    "                    if compiler.search(entry):\n",
    "                        return standardized_item, brand, category\n",
    "        return None, None, None\n",
    "\n",
    "    if isinstance(regex_container, list):\n",
    "        for standardized_service, compiler in regex_container:\n",
    "            if compiler.search(entry):\n",
    "                return standardized_service\n",
    "        return 'UNCATEGORIZED'\n",
    "                 \n",
    "def standardize_items(items, item_dict, exclude_words):\n",
    "    \"\"\"\n",
    "    Apply item name standardizations for free text in lists and strings.\n",
    "    Disregard accessories and other unrelated words captured in the exclude_words list.\n",
    "    Return standardized item name, brand, and category.\n",
    "    \"\"\"\n",
    "    standardized_items_list = []\n",
    "    brands_list = []\n",
    "    categories_list = []\n",
    "    \n",
    "    fallback = pd.Series({\n",
    "        'Item_Standardized': None, \n",
    "        'Brand': None, \n",
    "        'Category': None\n",
    "    })\n",
    "\n",
    "    # Do nothing if unable to have captured the unstandardized item name in the Data Loading Phase\n",
    "    if items is None:\n",
    "        pass\n",
    "        \n",
    "    if isinstance(items, list):\n",
    "        items = [item for item in items if not any(re.search(word, item) for word in exclude_words)]\n",
    "\n",
    "        # Guard against empty lists after cleanup\n",
    "        if not items:\n",
    "            return fallback\n",
    "\n",
    "        if len(items) > 1:\n",
    "            for item in items:\n",
    "                standardized_item, brand, category = return_standardizations(item, item_dict)\n",
    "                standardized_items_list.append(standardized_item)\n",
    "                brands_list.append(brand)\n",
    "                categories_list.append(category)\n",
    "                \n",
    "            return pd.Series({\n",
    "                'Item_Standardized': standardized_items_list,\n",
    "                'Brand': brands_list,\n",
    "                'Category': categories_list\n",
    "        })\n",
    "\n",
    "        else:\n",
    "            # Guard against one item in list after cleanup\n",
    "            items = items[0]\n",
    "            \n",
    "            standardized_item, brand, category = return_standardizations(items, item_dict)\n",
    "            \n",
    "            return pd.Series({\n",
    "                'Item_Standardized': standardized_item,\n",
    "                'Brand': brand,\n",
    "                'Category': category\n",
    "            })\n",
    "    \n",
    "    if isinstance(items, str):\n",
    "        if any(re.search(word, items) for word in exclude_words):\n",
    "            return fallback\n",
    "            \n",
    "        standardized_item, brand, category = return_standardizations(items, item_dict)\n",
    "            \n",
    "        return pd.Series({\n",
    "            'Item_Standardized': standardized_item,\n",
    "            'Brand': brand,\n",
    "            'Category': category\n",
    "        })\n",
    "        \n",
    "    return fallback\n",
    "\n",
    "def standardize_services(services, services_list, exclude_words_2):\n",
    "    \"\"\"\n",
    "    Apply service name standardizations for free text in lists and strings.\n",
    "    Disregard unrelated words captured in the exclude_words_2 list.\n",
    "    Return standardized service name.\n",
    "    \"\"\"\n",
    "    standardized_services_list = []\n",
    "\n",
    "    if services is None:\n",
    "        pass\n",
    "        \n",
    "    if isinstance(services, list):\n",
    "        services = [service for service in services if not any(re.search(word, service) for word in exclude_words_2)]\n",
    "\n",
    "        if not services:\n",
    "            return 'UNMAPPED'\n",
    "\n",
    "        if len(services) > 1:\n",
    "            for service in services:\n",
    "                standardized_service = return_standardizations(service, services_list)\n",
    "                standardized_services_list.append(standardized_service)\n",
    "            return standardized_services_list\n",
    "\n",
    "        else:\n",
    "            services = services[0]\n",
    "            standardized_service = return_standardizations(services, services_list)\n",
    "            return standardized_service\n",
    "\n",
    "    if isinstance(services, str):\n",
    "        if any(re.search(word, services) for word in exclude_words_2):\n",
    "            return 'UNMAPPED'\n",
    "        standardized_service = return_standardizations(services, services_list)\n",
    "        return standardized_service\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c679f802-d112-4e5a-a9c0-bed819ec3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup:\n",
    "\n",
    "# Load data:\n",
    "o_df = pd.read_json(r'C:\\Users\\shuju\\shus_workspace\\tannson_proj\\order_details.json')\n",
    "m_df = pd.read_json(r'C:\\Users\\shuju\\shus_workspace\\tannson_proj\\monetary_amts.json')\n",
    "\n",
    "# Create new identifier:\n",
    "m_df['Transaction Type'] = 'SALE'\n",
    "voided_transactions_indices = m_df.index[m_df['File Type'] == 'V']\n",
    "m_df.loc[voided_transactions_indices, 'Transaction Type'] = 'VOID' # Label voided transactions\n",
    "m_df = m_df.drop(columns='File Type')\n",
    "\n",
    "# Create new col for missing payment inferences:\n",
    "m_df['Explicit Payment Method'] = 'YES'\n",
    "move_col = m_df.pop('Explicit Payment Method')\n",
    "m_df.insert(2, 'Explicit Payment Method', move_col)\n",
    "\n",
    "# Correct out-of-bound date:\n",
    "o_df.loc[5119, 'Date'] = pd.to_datetime('2017-05-24')\n",
    "m_df.loc[5119, 'Date'] = pd.to_datetime('2017-05-24')\n",
    "\n",
    "# Correct Subtotal:\n",
    "m_df['Discount'] = m_df['Discount'].abs()\n",
    "m_df['Subtotal'] = m_df['Subtotal'] + m_df['Discount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa94006a-aa31-468a-982b-93375724bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct negative Total discrepancies:\n",
    "\n",
    "# Join Price and Service columns from o_df with m_df by indices\n",
    "temp_df = m_df.join(o_df[['Price', 'Service']])\n",
    "# Ensure no overlap with voided transactions\n",
    "temp_df = temp_df.iloc[temp_df.index.difference(voided_transactions_indices)]\n",
    "# Filter for negatives in Total\n",
    "negative_tots = temp_df[temp_df['Total'] < 0].copy()\n",
    "# Create mask for when customers are paid by Tannson\n",
    "mask_negatives = negative_tots.apply(\n",
    "    lambda row: paid_customers_fnc(row['Payment Method']) and n_price_coherence_fnc(row['Price']), axis=1\n",
    ")\n",
    "\n",
    "# Get indices and define appropriate labels\n",
    "negative_indices = negative_tots.index[mask_negatives]\n",
    "m_df.loc[negative_indices, 'Transaction Type'] = 'COMPANY PAID CUSTOMER'\n",
    "m_df.loc[negative_indices, 'Payment Method'] = 'CASH TO CUSTOMER'\n",
    "m_df.loc[negative_indices, 'Tax'] = 0\n",
    "m_df.loc[negative_indices, 'Subtotal'] = m_df.loc[negative_indices, 'Total']\n",
    "o_df.loc[negative_indices, 'Price'] = 0\n",
    "o_df.loc[negative_indices, 'Service'] = 'TRADE IN'\n",
    "\n",
    "# Correct exception cases based on manual inspection of files\n",
    "m_df.loc[2960, ['Subtotal', 'Total']] = 160\n",
    "m_df.loc[2960, 'Payment Method'] = 'PAID CASH'\n",
    "o_df.loc[2960, 'Price'] = 160\n",
    "\n",
    "m_df.loc[4115, ['Subtotal', 'Total']] = -30\n",
    "m_df.loc[4115, 'Transaction Type'] = 'COMPANY PAID CUSTOMER'\n",
    "m_df.loc[4115, 'Payment Method'] = 'CASH TO CUSTOMER'\n",
    "o_df.loc[4115, 'Price'] = 0\n",
    "o_df.loc[4115, 'Service'] = 'TRADE IN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f98fc8a8-8132-4010-ace3-dbc2974ac687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct zero Total discrepancies:\n",
    "\n",
    "# Filter for zeros in Total\n",
    "zero_tots = temp_df[temp_df['Total'] == 0].copy()\n",
    "# Create mask for no repairs\n",
    "mask_no_repairs = zero_tots.apply(\n",
    "    lambda row: base_fnc(row['Service'], no_repair_compiler), axis=1\n",
    ")\n",
    "\n",
    "# Get indices and define entries as no repairs\n",
    "no_repair_indices = zero_tots.index[mask_no_repairs]\n",
    "m_df.loc[no_repair_indices, 'Transaction Type'] = 'VOID'\n",
    "# o_df.loc[no_repair_indices, 'Service'] = 'DIAGNOSIS - NO REPAIR'\n",
    "\n",
    "# Create mask for courtesies\n",
    "mask_courtesies = zero_tots.apply(\n",
    "    lambda row: base_fnc(row['Service'], courtesy_compiler), axis=1\n",
    ")\n",
    "# Get indices and define entries as company courtesies\n",
    "# Note: Overlap in no_repair_indices checked for accuracy \n",
    "courtesy_indices = zero_tots.index[mask_courtesies]\n",
    "m_df.loc[courtesy_indices, 'Transaction Type'] = 'COMPANY COURTESY'\n",
    "m_df.loc[courtesy_indices, 'Payment Method'] = None\n",
    "\n",
    "# Filter for zero totals but relevant prices\n",
    "w_prices = zero_tots[zero_tots['Price'] != 0].copy()\n",
    "# Create mask for refunds, defined by abs. value of price == abs. value of refund amt\n",
    "mask_refunds = w_prices.apply(\n",
    "    lambda row: (\n",
    "        abs(\n",
    "            sum(row['Price']) if isinstance(row['Price'], list) # For list types\n",
    "            else row['Price'] # For scalars\n",
    "        )\n",
    "        == abs(row['Refund Amt'])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "# Get indices and void entries\n",
    "refund_indices = w_prices.index[mask_refunds]\n",
    "m_df.loc[refund_indices, 'Transaction Type'] = 'VOID'\n",
    "# Obtain remaining indices and set Price val as the Subtotal and Total val\n",
    "sum_prices_indices = w_prices.index.difference(refund_indices).to_numpy()\n",
    "sum_prices = w_prices.loc[sum_prices_indices, 'Price'].apply(\n",
    "    lambda cell: sum(cell) if isinstance(cell, list) else cell\n",
    ")\n",
    "m_df.loc[sum_prices_indices, ['Total', 'Subtotal']] = sum_prices\n",
    "\n",
    "# Label all remaining zero tots as needing examination\n",
    "exclude_indices = (\n",
    "    set(no_repair_indices) | set(courtesy_indices) | set(refund_indices) | set(sum_prices_indices)\n",
    ")\n",
    "include_indices = zero_tots.index.difference(exclude_indices).to_numpy()\n",
    "m_df.loc[include_indices, 'Transaction Type'] = 'FLAGGED — MISSING TOTAL'\n",
    "m_df.loc[include_indices, 'Explicit Payment Method'] = 'NO'\n",
    "\n",
    "m_df = m_df.drop('Refund Amt', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b5b61c-f17e-4754-808a-3c45e7740ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Payment Method:\n",
    "\n",
    "# Set no payment method for voided transactions\n",
    "m_df.loc[4371, 'Transaction Type'] = 'VOID'\n",
    "m_df.loc[m_df['Transaction Type'] == 'VOID', 'Payment Method'] = None\n",
    "\n",
    "sales = m_df[m_df['Transaction Type'].str.contains('SALE')].copy()\n",
    "\n",
    "# Standardize payment method strings\n",
    "cash_compiler = re.compile('CASH')\n",
    "mask_cash = sales.apply(\n",
    "    lambda row: base_fnc(row['Payment Method'], cash_compiler), axis=1\n",
    ")\n",
    "i_cash = sales.index[mask_cash]\n",
    "m_df.loc[i_cash, 'Payment Method'] = 'PAID CASH'\n",
    "\n",
    "cc_compiler = re.compile('CC|CREDIT|CARD|VISA|AMEX|AMX|AE')\n",
    "mask_cc = sales.apply(\n",
    "    lambda row: base_fnc(row['Payment Method'], cc_compiler), axis=1\n",
    ")\n",
    "i_cc = sales.index[mask_cc]\n",
    "m_df.loc[i_cc, 'Payment Method'] = 'PAID CC'\n",
    "\n",
    "check_compiler = re.compile('CHECK|CHK')\n",
    "mask_check = sales.apply(\n",
    "    lambda row: base_fnc(row['Payment Method'], check_compiler), axis=1\n",
    ")\n",
    "i_check = sales.index[mask_check]\n",
    "m_df.loc[i_check, 'Payment Method'] = 'PAID CHECK'\n",
    "\n",
    "# Guess missing payment methods\n",
    "m_inferred_pym = sales[sales['Payment Method'].isna()].copy()\n",
    "m_inferred_pym['Payment Method'] = m_inferred_pym.apply(infer_payment_fnc, axis=1)\n",
    "\n",
    "# Manual inspection of remaining files for payment method.\n",
    "# Transactions 3521, 5676 paid with cash.\n",
    "# Transaction 4371 voided.\n",
    "# Payment status of other transactions not mentioned.\n",
    "m_df.loc[[3521, 5676], 'Payment Method'] = 'PAID CASH'\n",
    "m_inferred_pym = m_inferred_pym.drop(index=[3521, 5676])\n",
    "m_inferred_pym.loc[m_inferred_pym['Payment Method'].isna(), 'Payment Method'] = 'UNCLASSIFIED'\n",
    "\n",
    "# Assign inferences to m_df\n",
    "m_inferred_pym_indices = m_inferred_pym.index.tolist()\n",
    "m_df.loc[m_inferred_pym_indices, 'Payment Method'] = m_inferred_pym['Payment Method']\n",
    "m_df.loc[m_inferred_pym_indices, 'Explicit Payment Method'] = 'NO'\n",
    "\n",
    "# Guess ambiguous payment methods\n",
    "a_inferred_pym = m_df[\n",
    "    ~(m_df['Payment Method'].str.contains('PAID CASH|PAID CC|PAID CHECK|UNCLASSIFIED', na=False)) \n",
    "    & (m_df['Transaction Type'].str.contains('SALE', na=False))\n",
    "    & (m_df['Payment Method'].notna())\n",
    "    ].copy()\n",
    "a_inferred_pym['Payment Method'] = a_inferred_pym.apply(infer_payment_fnc, axis=1)\n",
    "a_inferred_pym.loc[a_inferred_pym['Payment Method'].isna(), 'Payment Method'] = 'UNCLASSIFIED'\n",
    "\n",
    "# Assign inferences to m_df\n",
    "a_inferred_pym_indices = a_inferred_pym.index.tolist()\n",
    "m_df.loc[a_inferred_pym_indices, 'Payment Method'] = a_inferred_pym['Payment Method']\n",
    "m_df.loc[a_inferred_pym_indices, 'Explicit Payment Method'] = 'NO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c37d7aa5-a5d7-46e4-8e78-27969b8d9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct algebraic discrepancies:\n",
    "\n",
    "# Create new identifier for sales to define valid transactions in the business setting\n",
    "m_df.loc[m_df['Transaction Type'].str.contains('SALE', na=False), 'Business Logic Consistency'] = True\n",
    "# Set unclassified payment methods as being inapplicable for defining logic consistency\n",
    "m_df.loc[m_df['Payment Method'].str.contains('UNCLASSIFIED', na=False), 'Business Logic Consistency'] = None\n",
    "\n",
    "# Check for when Subtotal == Total w/o Tax, which violates paying by credit card rule\n",
    "not_cc_logic = m_df.index[\n",
    "    (m_df['Payment Method'].str.contains('PAID CC'))\n",
    "    & (np.isclose\n",
    "        (m_df['Subtotal'] - m_df['Discount'],\n",
    "         m_df['Total'], atol=0.02)\n",
    "      )\n",
    "    ]\n",
    "# Set transactions that violate the credit card rule to failing logic consistency\n",
    "m_df.loc[not_cc_logic, 'Business Logic Consistency'] = False\n",
    "\n",
    "# Filter for other algebraic discrepancies\n",
    "tot_mismatches = m_df[\n",
    "    (m_df['Transaction Type'] == 'SALE')\n",
    "    & (m_df['Business Logic Consistency'] == True)\n",
    "    & ~(np.isclose\n",
    "        (m_df['Subtotal'] - m_df['Discount'] + m_df['Tax'],\n",
    "         m_df['Total'], atol=0.02)\n",
    "       )\n",
    "    ].copy()\n",
    "\n",
    "# Add Price column\n",
    "tot_mismatches = tot_mismatches.join(o_df['Price'])\n",
    "\n",
    "# Create mask where there should be zero tax\n",
    "mask_zero_tax = tot_mismatches.apply(\n",
    "    lambda row: (\n",
    "        (base_fnc(row['Payment Method'], no_tax_compiler)) # Match on CASH or CHECK payments\n",
    "        or (row['Subtotal'] == row['Total']) # Capture when Subtotal == Total\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Set tax values that match mask_zero_tax conditions to 0\n",
    "zero_tax_indices = tot_mismatches.index[mask_zero_tax]\n",
    "m_df.loc[zero_tax_indices, 'Tax'] = 0\n",
    "tot_mismatches.loc[mask_zero_tax, 'Tax'] = 0\n",
    "\n",
    "# Compute algebraic discrepancies again (2)\n",
    "tot_mismatches = tot_mismatches[\n",
    "    ~np.isclose(\n",
    "        tot_mismatches['Subtotal']\n",
    "        - tot_mismatches['Discount']\n",
    "        + tot_mismatches['Tax'],\n",
    "        tot_mismatches['Total'],\n",
    "        atol=0.02\n",
    "    )\n",
    "    ]\n",
    "\n",
    "# Use subtotal as total and total as subtotal if valid\n",
    "tot_mismatches[['Subtotal', 'Total']] = tot_mismatches.apply(correct_subs_and_tots, axis=1)\n",
    "\n",
    "# Transactions remain algebraically inconsistent after manual inspection ->\n",
    "# Set remaining algebraic discrepancies as failing logic consistency\n",
    "tot_mismatches.loc[\n",
    "    ~np.isclose(\n",
    "        tot_mismatches['Subtotal'] \n",
    "        - tot_mismatches['Discount'] \n",
    "        + tot_mismatches['Tax'], \n",
    "        tot_mismatches['Total'], \n",
    "        atol=0.02\n",
    "    ), 'Business Logic Consistency'] = False\n",
    "\n",
    "# Get indices of previous transactions and reassign altered columns to m_df\n",
    "fin_mismatches_indices = tot_mismatches.index.tolist()\n",
    "m_df.loc[fin_mismatches_indices, ['Subtotal', 'Total']] = tot_mismatches[['Subtotal', 'Total']]\n",
    "m_df.loc[fin_mismatches_indices, 'Business Logic Consistency'] = tot_mismatches['Business Logic Consistency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee45b4aa-b63f-433e-8be0-96e00bdc8a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Service_Standardized\n",
       "SCREEN REPLACEMENT OR REPAIR                  3175\n",
       "UNCATEGORIZED                                 1003\n",
       "SOFTWARE RESTORATION                           560\n",
       "STARTUP REPAIR                                 512\n",
       "BATTERY REPLACEMENT OR REPAIR                  499\n",
       "UNMAPPED                                       435\n",
       "CHARGING PORT REPLACEMENT OR REPAIR            306\n",
       "HARDWARE DIAGNOSTICS                           276\n",
       "DATA TRANSFER/BACKUP                           252\n",
       "MISC. HARDWARE REPLACEMENT OR REPAIR           248\n",
       "SOFTWARE INSTALLATION OR UPDATE                225\n",
       "GENERAL DIAGNOSTICS                            170\n",
       "DEVICE OR SERVICE UNLOCK                       141\n",
       "CHARGING/POWER DIAGNOSTICS                     120\n",
       "STORAGE DRIVE REPLACEMENT OR REPAIR            112\n",
       "COVER REPLACEMENT OR REPAIR                    110\n",
       "MALWARE REMOVAL                                106\n",
       "SOFTWARE DIAGNOSTICS                            74\n",
       "SYSTEM CUSTOMIZATION                            71\n",
       "PERSONAL ACCOUNT SERVICE                        69\n",
       "BUTTON REPLACEMENT OR REPAIR                    67\n",
       "HARDWARE INSTALLATION OR UPGRADE                54\n",
       "DATA RECOVERY                                   32\n",
       "POWER SUPPLY/ADAPTER REPLACEMENT OR REPAIR      32\n",
       "SOFTWARE UNINSTALLATION OR REMOVAL              29\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize Item and Service:\n",
    "\n",
    "o_df_standardized = o_df.copy()\n",
    "o_df_standardized[['Item_Standardized', 'Brand', 'Category', 'Service_Standardized']] = 'NEW'\n",
    "o_df_standardized[['Item_Standardized', 'Brand', 'Category']] = o_df_standardized['Item'].apply(lambda row: standardize_items(row, tannson_re.item_dict, tannson_re.exclude_words))\n",
    "o_df_standardized['Service_Standardized'] = o_df_standardized['Service'].apply(lambda row: standardize_services(row, tannson_re.services_list, tannson_re.exclude_words_2))\n",
    "\n",
    "new_order = [\n",
    "    'Item_Standardized', 'Brand', 'Category', 'Service', 'Service_Standardized', 'Price'\n",
    "]\n",
    "o_df_standardized = o_df_standardized[new_order]\n",
    "\n",
    "o_df_standardized = o_df_standardized.explode('Service_Standardized')\n",
    "# o_df_standardized = o_df_standardized.explode('Item_Standardized')\n",
    "o_df_standardized['Service_Standardized'].value_counts(dropna=False).sum()\n",
    "o_df_standardized.groupby('Service_Standardized', dropna=False).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c4a16-1a43-420a-b816-1d8ff1a6ae6d",
   "metadata": {},
   "source": [
    "12/31/2025:\n",
    "* 332 of 421 zero totals fixed. **Remaining 89** requires manual inspection or inquiry to client.\n",
    "\n",
    "01/01/2026:\n",
    "* 991 of 1076 algebraic discrepancies in totals fixed. Remaining: 85.\n",
    "\n",
    "01/03/2026:\n",
    "* Created 'Explicit Payment Type' and identified VOIDs. Tomorrow: Infer missing payment method fnc (CASH if zero or null tax and CC if tax), get indices of inferences, set EPM to 'NO'. Note: Example of reconstructing variables from noisy observational data.\n",
    "\n",
    "01/04/2026:\n",
    "* Created CASH/CHECK or CC inference fnc. Standardized payment method strings. Need to: 1) Incorporate DIAGNOSTIC and COURTESY key words found in Payment column to mask_no_repairs and mask_courtesies. 2) Standardize strings in lists (if list contains CASH, return CASH as the sole value instead of the list — might not need to create a new fnc.)\n",
    "\n",
    "01/06/2026:\n",
    "* Classified payment methods of all sales.\n",
    "\n",
    "01/08/2026:\n",
    "* Classified refunds as VOID. Set Price col value or sum of Price col values as Total.\n",
    "\n",
    "01/11/2026:\n",
    "* 1071 of 1078 algebraic discrepancies in totals fixed. **Remaining: 7**. All other entries are algebraically unsound even with manual inspection, so mark as violating business logic consistency.\n",
    "\n",
    "01/18/2026:\n",
    "* Standardized: ASUS, APPLE, SAMSUNG.\n",
    "\n",
    "01/19/2026:\n",
    "* Created function to apply item name standardizations.\n",
    "\n",
    "01/20/2026:\n",
    "* Standardized: GOOGLE, MICROSOFT, NOKIA, LG.\n",
    "\n",
    "01/21/2026:\n",
    "* Standardized: TOSHIBA, HP, LENOVO.\n",
    "\n",
    "01/22/2026:\n",
    "* Standardized: DELL, SONY.\n",
    "\n",
    "01/25/2026:\n",
    "* Standardized: ACER, AMAZON, HUAWEI, MOTOROLA, GATEWAY.\n",
    "\n",
    "01/26/2026:\n",
    "* Standardized: ALCATEL, ALIENWARE, TMOBILE PRODUCTS, CYBERPOWERPC, MSI, ONSITE SERVICES. Total: ~96% standardized. Remaining 4% grouped to OTHER.\n",
    "\n",
    "01/27/2026:\n",
    "* Initial standardization of top 25 services.\n",
    "\n",
    "02/01/2026:\n",
    "* 64.4% of Services standardized.\n",
    "\n",
    "02/04/2026:\n",
    "* Turned base standardization fnc into a wrapper + created service standardization fnc.\n",
    "\n",
    "02/12/2026:\n",
    "* Fully standardized ~83% of services. ~5% of services not captured upon initial loading of data. Chose to not standardize remaining ~12% of services because of high free text variability. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
